
<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  ?????????????????????????????????????????????????? -->
  <meta charset="utf-8">
  <title>Artificial Neural Network</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  ?????????????????????????????????????????????????? -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  ?????????????????????????????????????????????????? -->
 <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600'  rel='stylesheet' type='text/css'>

  <!-- CSS
  ?????????????????????????????????????????????????? -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Scripts
  ?????????????????????????????????????????????????? -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  <script src="js/site.js"></script>

  <!-- Favicon
  ?????????????????????????????????????????????????? -->
  <link rel="icon" type="image/png" href="dist/images/favicon.png">





<style>

</style>






</head>

<body class="code-snippets-visible">

  <!-- Primary Page Layout
  ?????????????????????????????????????????????????? -->
  <div class="container">


    <div class="navbar-spacer"></div>
   <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="#intro">Home</a></li>
          <li class="navbar-item">
            <a class="navbar-link" href="#" data-popover="#codeNavPopover">MLB API</a>
            <div id="codeNavPopover" class="popover">
              <ul class="popover-list">
                <li class="popover-item">
                  <a class="popover-link" href="#intro">Intro</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#json">JSON Response</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#client">Node Client</a>
                </li>
              </ul>
            </div>
          </li>
          <li class="navbar-item"><a class="navbar-link" href="neuralnetwork.html">Artificial Neural Network</a></li>
        </ul>
      </div>
    </nav>

 
          <!-- Why use-->
          <div class="docs-section" style="padding-top: 6rem;padding-bottom: 3rem;" id="intro">
              <h3>Artificial Neural Network</h3>
	    
              <p>The following code is a simple three layer neural network written in Python and NumPy.  This was my first shot at programming a neural network, so there may be some issues with it. 
		      I wanted to build it from scratch so I could attempt to understand and conceptualize the math involved.  </p></p>
	  A neural network is a learning algorithm that attempts to solve complex problems in computer vision, speech recognition, and other fields in artificial intelligence.  Artificial Neural Networks were named after their similarity to the human brain, with its millions of neurons and axons receiving 
	  inputs from the external environment and creating electrical impulses which travel through the connected network of the brain. </p>
	  <div>My program will have a single hidden layer.  A simple visualization is below.  Source code is <a href="https://github.com/mattgorb/MLBFantasyNeuralNet">here</a></div>
	<div><img src="mlb pics/NeuralNetwork.png" width="100%" height="5%"/></div>
	
		  <h4>Data</h4>
		  <p>I'm going to be using NumPy here, which is a linear algebra library for Python.  I read all my data from csv.  A basic function of this is below which creates two matrices, inputData and outputData.  The output data is in the fifth column of the csv file and the input data is from the sixth column to the last column.</p>
	 
<pre><code>
#Basic, stripped down data load from a CSV file to two NumPy arrays
def LoadData(fileName, start,end):
    inputData=[] #python array
    outputData=[]    
    with open(fileName, 'rb') as csvfile: 
        reader = csv.reader(csvfile, delimiter=',')
        for row in itertools.islice(reader, start,end): #itertools is a python iterator tool
            output=np.longfloat(row[5]) #output is the fifth index in each row
	    #create a numpy long float array for each row from the sixth index to the last (len(row)) index 
            input = [np.longfloat(x) for x in itertools.islice(row , 6,len(row) ) if x != ''] 
	    #add each row to the python array
            inputData.append(input)
            outputData.append(output)
    return inputData,outputData			
			</code></pre>
              </div>
<p>Note that the input data should already be normalized.  I'm not going to talk about this, but it is a very important piece to building a working neural network.   </p>
	<p>Links on data normalization: </br>
<a href="https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks">Data Normalization in Neural Networks</a></br>
<a href="https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx">How to Standardize data for Neural Networks</a>

<p>Another important facet of supervised learning is the three different types of datasets: </br>
Training set:  The set to send through your model so that it can learn.  The model pairs the input with the expected output and learns what the output should be given the inputs.  The "weights" of the neurons get updated so that they can most perfectly reflect what the output will be</br></p> 
Validation set: 
Test set:  The dataset you use once your training is complete to see if your model is really as smart as its validation set thinks it is.  
</p>	
	
	


	
	
	
	
		<div class="docs-section" style={intro} id="runScript">
		<h5 style={{marginBottom: 0}} >Training </h5>
			<div>The training method is the meat of the project.  This is where backpropagation and several other neural network techniques are implemented.  Here is the code with comments explaining the functionality. </div>
		<pre><code> 
def Train(self):
    global weights0,weights1,layer1_deltaPrev,layer2_deltaPrev
    
    #training iterations are set by the user in the program initialization. 
    for iter in xrange(self.trainingIterations):
   
   	#layer 0 equals the training inputs in NumPy matrix form.  
        layer0 = self.TrainingIn
        
        #neural nets should always use shuffle.  One big bug in this program is that the outputs are in a separate
        #matrix.  This means that randomly shuffling the inputs is going to give them random outputs.  For my training
        #I set shuffle to off.  
        if(self.shuffle):
	    np.random.shuffle(layer0)

	#np.dot is a NumPy multiplication function.  It multiplies layer0(input) by the randomly initialized weights, and is fed through the sigmoid function self.nonlin
        layer1 = self.nonlin(np.dot(layer0,self.weights0))

	#you should always include a bias layer. This means adding a column of 1's to your matrix 
        if(self.biasLayer1TF):
            self.biaslayer1 = np.ones(layer1.shape[0])
            layer1=np.column_stack((layer1,self.biaslayer1))
            
        #dropout should also always be used.  0>dropout<1
        if(self.dropout):
            layer1 *= np.random.binomial([np.ones((layer1.shape[0],layer1.shape[1]))],1-self.dropoutPercent)[0] * (1.0/(1-self.dropoutPercent))
        
        #layer2 equals layer1*weights1 fed through the sigmoid function self.nonlin
        #layer2 is what the network is predicting the output will be.  
        layer2 = self.nonlin(np.dot(layer1,self.weights1))
        
        #more on this later
        if(self.miniBtch_StochastGradTrain):
            self.layer1_delta,self.layer2_delta=self.MiniBatchOrStochasticGradientDescent()
            self.layer2_deltaPrev=self.layer2_delta
            self.layer1_deltaPrev=self.layer1_delta
        else:

	    #the error= the distance between the guesses from the actual values.  
            layer2_error = self.TrainingOut - layer2
            
            #take the derivative (slope) of each node in layer2, and multiply it by the error.  The higher the error,
            #the more the slope delta will be.  Learning rate is also applied here, which is how fast you want your network to correct 
            #itself.  Additionally momentum is applied to the end of this, which is added the previous iterations deltas to the current iteration 
            self.layer2_delta = layer2_error*self.nonlin(layer2,deriv=True)*self.learningRate+self.momentum*self.layer2_deltaPrev
            self.layer2_deltaPrev=self.layer2_delta

	    #.T means the transposed matrix in Numpy.  
            layer1_error = self.layer2_delta.dot(self.weights1.T)
            self.layer1_delta = layer1_error * self.nonlin(layer1,deriv=True)*self.learningRate+self.momentum*self.layer1_deltaPrev
            self.layer1_deltaPrev=self.layer1_delta

	#update the weights in each layer with the delta layers
        self.weights1 += layer1.T.dot(self.layer2_delta)
        if(self.biasLayer1TF):
            self.weights0 += layer0.T.dot(self.layer1_delta.T[:-1].T)
        else:
            self.weights0 += layer0.T.dot(self.layer1_delta)
        

        if (iter% 20) == 0:
            self.CalculateValidationError()

            if(self.minimumValidationError<0.01):
                break
		</code></pre>









              <div class="docs-section" style={intro} id="client">
                  <h2 style={{marginBottom: 0}} >Example</h2>
		      <div>I'm going to walkthrough the neural net code above with the following data:</div>
		       <div><img src="mlb pics/input.png" width="20%" height="5%"/></div>
              
		      			<p>Looking at the table we have 4 rows with 3 variables of input data.  The output data is 4 rows with 1 result. 
				So we will have a TestIn matrix of size 4x3 and a TestOut matrix of 4x1.    
				The above table looks like the following in a NumPy array:</br>
			TestIn = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ]) </br>
			TestOut = np.array([[0,1,1,0]]).T	</br>
			Note the ".T" in the TestOut variable.  This means transpose the matrix.  It is visualized here as a 1x4 matrix, by transposing it we create a 4X1 matrix, conducive to our input set.</br></br> </p>
		Since we are working with such a simple dataset I'm going to skip over validation data and test data.  Our test data will simply be our training data.  

<p>Now that we have input data to send to the network, I'm going set the parameters for the network. I do this in main.py.  These parameters get sent to the Neural Network class and initialize the model. </br>
		I'm going to concentrate on the parameters hiddenDim, learningRate, trainingIterations, momentum, and dropout. </br></br>Below is the parameter dictionary I send to the Neural Network class.    </p>
		      <p> </p>
		      
		  	<pre><code>
Run({"hiddenDim":2,"learningRate":.05,"trainingIterations":5000,"momentum":.1,
"dropout":True,"dropoutPercent":0.65,
 "miniBtch_StochastGradTrain":"False","miniBtch_StochastGradTrain_Split":null,"NFoldCrossValidation":False,"N":null#skip these,
 "biasLayer1TF":True,"addInputBias":False,"shuffle":False
})	
			</code></pre>
					      
		       <p>For now I'm going to skip over bias, minibatch training, stochastic gradient descent training, and N-fold cross validation.  I'll talk about these concepts later.  </p>

			
			
			</div>
<h5>Initializing weights and delta previous</h5>
<div>In my NeuralNet class __init__ code I initialize the weights based on the hiddenDim variable: 
<pre><code>np.random.seed(1)
self.weights0 = (2*np.random.random((self.TrainingIn.shape[1],self.hiddenDim)) - 1)
self.weights1 = (2*np.random.random((self.hiddenDim+1 if self.biasLayer1TF else self.hiddenDim,1)) - 1)
</code></pre>
	<p>
	np.random.random creates a matrix of random numbers between 0 and 1.  It is best practice to initialize the weights randomly with mean 0, so I multiply these numbers by 2 and subtract 1.  </br>
self.TrainingIn.shape returns the size of the matrix, in our case 4x3.  Therefore self.TrainingIn.shape[1] returns the second number of this value, 3.  </br>
hiddenDim was set to 2 in our initalization parameters above.  So self.weights0 will be a matrix of size 3X2 with random numbers between -1 and 1.  </br>

Since bias is turned off for this example, weights1 will be initialized with a matrix size 2(hiddenDim)x1 with random numbers between -1 and 1.  </br></br>
We also need to intialize "layerx_deltaPrev" matrices for momentum.  </br>np.zeros creates a matrix of 0's.  
<pre><code>self.layer2_deltaPrev=np.zeros(shape=(self.TrainingOut.shape[0],self.weights1.shape[1]))
self.layer1_deltaPrev=np.zeros(shape=(self.TrainingIn.shape[0],self.hiddenDim+1 if self.biasLayer1TF else self.hiddenDim))
</code></pre>

self.layer2_deltaPrev will be a matrix of size 4X1, and self.layer1_deltaPrev will be a matrix of size 4x2. 


	</p>
</div>

<h5>Training</h5>
<div>Now we're ready to start training.  </br>


</div>
					      <p>Shuffle:  Randomly rearrange the data rows so that the network is fed new combinations of data on each iteration. </p>






<!-- End Document
  ?????????????????????????????????????????????????? -->
</body>
</html>
