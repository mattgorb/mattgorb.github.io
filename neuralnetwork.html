
<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  �������������������������������������������������� -->
  <meta charset="utf-8">
  <title>Artificial Neural Network</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  �������������������������������������������������� -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  �������������������������������������������������� -->
 <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600'  rel='stylesheet' type='text/css'>

  <!-- CSS
  �������������������������������������������������� -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Scripts
  �������������������������������������������������� -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  <script src="js/site.js"></script>

  <!-- Favicon
  �������������������������������������������������� -->
  <link rel="icon" type="image/png" href="dist/images/favicon.png">





<style>

</style>






</head>

<body class="code-snippets-visible">

  <!-- Primary Page Layout
  �������������������������������������������������� -->
  <div class="container">


    <div class="navbar-spacer"></div>
   <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
          <li class="navbar-item"><a class="navbar-link" href="#intro">Home</a></li>
          <li class="navbar-item">
            <a class="navbar-link" href="#" data-popover="#codeNavPopover">MLB API</a>
            <div id="codeNavPopover" class="popover">
              <ul class="popover-list">
                <li class="popover-item">
                  <a class="popover-link" href="#intro">Intro</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#json">JSON Response</a>
                </li>
                <li class="popover-item">
                  <a class="popover-link" href="#client">Node Client</a>
                </li>
              </ul>
            </div>
          </li>
          <li class="navbar-item"><a class="navbar-link" href="neuralnetwork.html">Artificial Neural Network</a></li>
        </ul>
      </div>
    </nav>

 
          <!-- Why use-->
          <div class="docs-section" style="padding-top: 6rem;padding-bottom: 3rem;" id="intro">
              <h3>Artificial Neural Network</h3>
	    
              <p>The following code is a simple three layer neural network written in Python and NumPy.  This was my first shot at programming a neural network, so there may be some issues with it. 
		      I wanted to build it from scratch so I could attempt to understand and conceptualize the math involved.  </p></p>
	  A neural network is a learning algorithm that attempts to solve complex problems in computer vision, speech recognition, and other fields in artificial intelligence.  Artificial Neural Networks were named after their similarity to the human brain, with its millions of neurons and axons receiving 
	  inputs from the external environment and creating electrical impulses which travel through the connected network of the brain. </p>
	  <div>My program will have a single hidden layer.  A simple visualization is below.  Source code is <a href="https://github.com/mattgorb/MLBFantasyNeuralNet">here</a></div>
	<div><img src="mlb pics/NeuralNetwork.png" width="100%" height="5%"/></div>
	
		  <h4>Data</h4>
		  <p>I'm going to be using NumPy here, which is a linear algebra library for Python.  I read all my data from csv.  A basic function of this is below which creates two matrices, inputData and outputData.  The output data is in the fifth column of the csv file and the input data is from the sixth column to the last column.</p>
	 
<pre><code>
#Basic, stripped down data load from a CSV file to two NumPy arrays
def LoadData(fileName, start,end):
    inputData=[] #python array
    outputData=[]    
    with open(fileName, 'rb') as csvfile: 
        reader = csv.reader(csvfile, delimiter=',')
        for row in itertools.islice(reader, start,end): #itertools is a python iterator tool
            output=np.longfloat(row[5]) #output is the fifth index in each row
	    #create a numpy long float array for each row from the sixth index to the last (len(row)) index 
            input = [np.longfloat(x) for x in itertools.islice(row , 6,len(row) ) if x != ''] 
	    #add each row to the python array
            inputData.append(input)
            outputData.append(output)
    return inputData,outputData			
			</code></pre>
              </div>
<p>Note that the input data should already be normalized.  I'm not going to talk about this, but it is a very important piece to building a working neural network.   </p>
	<p>Links on data normalization: </br>
<a href="https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks">Data Normalization in Neural Networks</a></br>
<a href="https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx">How to Standardize data for Neural Networks</a>

<p>Another important facet of supervised learning is the three different types of datasets: </br>
Training set:  The set to send through your model so that it can learn.  The model pairs the input with the expected output and learns what the output should be given the inputs.  The "weights" of the neurons get updated so that they can most perfectly reflect what the output will be</br></p> 
Validation set: 
Test set:  The dataset you use once your training is complete to see if your model is really as smart as its validation set thinks it is.  
</p>	
	
	
              <div class="docs-section" style={intro} id="client">
                  <h5 style={{marginBottom: 0}} >Main.py</h5>
		      <p>We'll start in main with the following code:</p>
		      
		  	<pre><code>
#NetworkParameters examples
"""hiddenDim=5 dropout=True dropoutPercent=.1 learningRate=.01 momentum=0.01 
miniBtch_StochastGradTrain=True miniBtch_StochastGradTrain_Split=30 #for stochastic training set miniBtch_StochastGradTrain="Stochastic"
addInputBias=True biasLayer1TF=True trainingIterations=500 NFoldCrossValidation=True 
N=5 If you want Nfold By dates set N='Dates'
"""


Run({"hiddenDim":2,
"learningRate":.005,
"miniBtch_StochastGradTrain":"Stochastic","miniBtch_StochastGradTrain_Split":250,
"trainingIterations":5000,
"momentum":.1,
"dropout":True,"dropoutPercent":0.65,
"biasLayer1TF":True,"addInputBias":True,
"NFoldCrossValidation":False,#"N":'Dates',
"N":5,
"shuffle":True
}
)	
			
			</code></pre>
		       <p>These are all the network parameters I've built in.  They are all fundamental concepts of neural nets.  </p>
		      <p></p>
		      <p></p>
		      <p></p>
		      <p>trainingIterations: >1.  </p>
		      <p></p>
		      <p>biasLayer1TF: True or False. addInputBias: True or False  </p>
		      <p></p>
		      <p></p>
		      <p>Shuffle:  True or False.  Randomly rearrange the data rows so that the network is fed new combinations of data on each iteration. </p>
              </div>
			                    <div class="docs-section" style={intro} id="client">
                  <h5 style={{marginBottom: 0}} >Sample NodeJs Client:</h5>
		  	<pre><code></code></pre>
              </div>
	
	
	
	
		<div class="docs-section" style={intro} id="runScript">
		<h5 style={{marginBottom: 0}} >Training </h5>
			<div>The training method is the meat of the project.  This is where backpropagation and several other neural network techniques are implemented.  Here is the code with comments explaining the functionality. </div>
		<pre><code> 
def Train(self):
    global weights0,weights1,layer1_deltaPrev,layer2_deltaPrev
    
    #training iterations are set by the user in the program initialization.  This allows me to experiment. 
    for iter in xrange(self.trainingIterations):
   
   			#layer 0 equals the training inputs in NumPy matrix form.  
        layer0 = self.TrainingIn
        
        #neural nets should always use shuffle.  One big bug in this program is that the outputs are in a separate
        #matrix.  This means that randomly shuffling the inputs is going to give them random outputs.  For my training
        #I set shuffle to off.  
        if(self.shuffle):
	    np.random.shuffle(layer0)

	#np.dot is a NumPy multiplication function.  It multiplies layer0(input) by the randomly initialized weights, and is fed through the sigmoid function self.nonlin
        layer1 = self.nonlin(np.dot(layer0,self.weights0))

	#you should always include a bias layer. This means adding a column of 1's to your matrix 
        if(self.biasLayer1TF):
            self.biaslayer1 = np.ones(layer1.shape[0])
            layer1=np.column_stack((layer1,self.biaslayer1))
            
        #dropout should also always be used.  0>dropout<1
        if(self.dropout):
            layer1 *= np.random.binomial([np.ones((layer1.shape[0],layer1.shape[1]))],1-self.dropoutPercent)[0] * (1.0/(1-self.dropoutPercent))
        
        #layer2 equals layer1*weights1 fed through the sigmoid function self.nonlin
        #layer2 is what the network is predicting the output will be.  
        layer2 = self.nonlin(np.dot(layer1,self.weights1))
        
        #more on this later
        if(self.miniBtch_StochastGradTrain):
            self.layer1_delta,self.layer2_delta=self.MiniBatchOrStochasticGradientDescent()
            self.layer2_deltaPrev=self.layer2_delta
            self.layer1_deltaPrev=self.layer1_delta
        else:

	    #the error= the distance between the guesses from the actual values.  
            layer2_error = self.TrainingOut - layer2
            
            #take the derivative (slope) of each node in layer2, and multiply it by the error.  The higher the error,
            #the more the slope delta will be.  Learning rate is also applied here, which is how fast you want your network to correct 
            #itself.  Additionally momentum is applied to the end of this, which is added the previous iterations deltas to the current iteration 
            self.layer2_delta = layer2_error*self.nonlin(layer2,deriv=True)*self.learningRate+self.momentum*self.layer2_deltaPrev
            self.layer2_deltaPrev=self.layer2_delta

	    #.T means the transposed matrix in Numpy.  
            layer1_error = self.layer2_delta.dot(self.weights1.T)
            self.layer1_delta = layer1_error * self.nonlin(layer1,deriv=True)*self.learningRate+self.momentum*self.layer1_deltaPrev
            self.layer1_deltaPrev=self.layer1_delta

	#update the weights in each layer with the delta layers
        self.weights1 += layer1.T.dot(self.layer2_delta)
        if(self.biasLayer1TF):
            self.weights0 += layer0.T.dot(self.layer1_delta.T[:-1].T)
        else:
            self.weights0 += layer0.T.dot(self.layer1_delta)
        

        if (iter% 20) == 0:
            self.CalculateValidationError()

            if(self.minimumValidationError<0.01):
                break
		</code></pre>









              <div class="docs-section" style={intro} id="client">
                  <h5 style={{marginBottom: 0}} >Example</h5>
		       <div><img src="mlb pics/input.png" width="100%" height="5%"/></div>
		  	<pre><code></code></pre>
              </div>








<!-- End Document
  �������������������������������������������������� -->
</body>
</html>
