
<!DOCTYPE html>
<html lang="en">
<head>

  <!-- Basic Page Needs
  ?????????????????????????????????????????????????? -->
  <meta charset="utf-8">
  <title>Artificial Neural Network</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  ?????????????????????????????????????????????????? -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  ?????????????????????????????????????????????????? -->
 <link href='https://fonts.googleapis.com/css?family=Raleway:400,300,600'  rel='stylesheet' type='text/css'>

  <!-- CSS
  ?????????????????????????????????????????????????? -->
  <link rel="stylesheet" href="css/normalize.css">
  <link rel="stylesheet" href="css/skeleton.css">
  <link rel="stylesheet" href="css/custom.css">

  <!-- Scripts
  ?????????????????????????????????????????????????? -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>
  <script src="https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js"></script>
  <script src="js/site.js"></script>

  <!-- Favicon
  ?????????????????????????????????????????????????? -->
  <link rel="icon" type="image/png" href="dist/images/favicon.png">





<style>

</style>






</head>

<body class="code-snippets-visible">

  <!-- Primary Page Layout
  ?????????????????????????????????????????????????? -->
  <div class="container">


    <div class="navbar-spacer"></div>
   <nav class="navbar">
      <div class="container">
        <ul class="navbar-list">
		<li class="navbar-item"><a class="navbar-link" href="dailymlblineups.html">MLB API</a></li>
		<li class="navbar-item"><a class="navbar-link" href="neuralnetwork.html">Artificial Neural Network</a></li>
		<li class="navbar-item"><a class="navbar-link" href="nearestneighbor.html">Nearest Neighbors</a></li>
        </ul>
      </div>
    </nav>

 
          <!-- Why use-->
          <div class="docs-section" style="padding-top: 6rem;padding-bottom: 3rem;" id="intro">
              <h3>Artificial Neural Network</h3>
	    
              <p>The following code is a simple three layer neural network written in Python and NumPy.  This was my first shot at programming a neural network, so there may be some issues with it. 
		      I wanted to build it from scratch so I could attempt to understand and conceptualize the math involved.  </p></p>
	  A neural network is a learning algorithm that attempts to solve complex problems in computer vision, speech recognition, and other fields in artificial intelligence.  Artificial Neural Networks were named after their similarity to the human brain, with its millions of neurons and axons receiving 
	  inputs from the external environment and creating electrical impulses which travel through the connected network of the brain. </p>
	  <div>My program will have a single hidden layer.  A simple visualization is below.  <a   href="https://github.com/mattgorb/MLBFantasyNeuralNet">Source code is here</a></div>
	<div><img src="mlb pics/NeuralNetwork.png" width="100%" height="5%"/></div>
	
	
	

	
	
		  <h4>Data</h4>
		  <p>I'm going to be using NumPy here, which is a linear algebra library for Python.  I read all my data from csv.  A basic function of this is below which creates two matrices, inputData and outputData.  The output data is in the fifth column of the csv file and the input data is from the sixth column to the last column.</p>
	 
<pre><code>
#Basic, stripped down data load from a CSV file to two NumPy arrays
def LoadData(fileName, start,end):
    inputData=[] #python array
    outputData=[]    
    with open(fileName, 'rb') as csvfile: 
        reader = csv.reader(csvfile, delimiter=',')
        for row in itertools.islice(reader, start,end): #itertools is a python iterator tool
            output=np.longfloat(row[5]) #output is the fifth index in each row
	    #create a numpy long float array for each row from the sixth index to the last (len(row)) index 
            input = [np.longfloat(x) for x in itertools.islice(row , 6,len(row) ) if x != ''] 
	    #add each row to the python array
            inputData.append(input)
            outputData.append(output)
    return inputData,outputData			
			</code></pre>
              </div>
<p>Note that the input data should already be normalized.  I'm not going to talk about this, but it is a very important piece to building a working neural network.   </p>
	<p>Links on data normalization: </br>
<a href="https://stats.stackexchange.com/questions/7757/data-normalization-and-standardization-in-neural-networks">Data Normalization in Neural Networks</a></br>
<a href="https://visualstudiomagazine.com/articles/2014/01/01/how-to-standardize-data-for-neural-networks.aspx">How to Standardize data for Neural Networks</a>

<p>Another important facet of supervised learning is the three different types of datasets.  I'll go over these below. </p>	
	
	<h4>Stats and graphs and stuffs</h4>
<div><img src="mlb pics/underfitting-overfitting.png" width="70%" height="5%"/></div>
		      <div>Think of the dots above as data.  We are trying to learn from the data and create a model so that we can use it in the future for some particular use.  Here the model is the line.  </br>For example, suppose the input data is the the x coordinate and the output data is the y coordinate.  If we want to predict the y coordinate given the x coordinate, we can use the line to predict where the dot will be. </br>
In statistcs, overfitting is making your model too complex to have a valuable predictive performance.  The overfitting line may look like it has a perfect guesses in the picture above, but in reality when an overfit model is given new inputs it makes poor predictions.  This is because it learns from the stochastic error of the data, which is the error that is random from one measurement to the next.  This causes the model to overreact to minor fluctuations in the data.  

</div>	
<div><img src="mlb pics/ModelError.png" width="50%" height="5%"/></div>
<div>
This is another way to look at overfitting.  The bigger graph at the bottom shows model complexity vs. model prediction error.  As the complexity of the model increases, the risk of overfitting goes up.  A lot of times you won't be able to tell you are overfitting your model unless you use new data that hasn't been trained on the model.  This is called the validation set.    A good way to tell if you are overfitting is measuring the training data vs the validation data.  If the model error, or model accuracy, is roughly the same and improving at the same rate, you are on the right track.  </br></br>
The next image shows how a model's accuracy can be depicted graphically.  Note how the green green validatino line is near the training set accuracy while the blue line is a model with overfitting.   </div>
<div><img src="mlb pics/overfit.jpg" width="40%" height="5%"/></div>	<div>
In the case of neural networks, overfitting often occurs when there are too many nodes.  Each node will attempt to hold specific information about the input data, and when there are too many nodes, they will attempt to hold information on the data that is too specific and can't be generalized to new data.  </div>	
		<div class="docs-section" style={intro} id="runScript">
		<h3 style={{marginBottom: 0}} >Training </h3>
			<div>The training method is the meat of the project.  This is where backpropagation and several other neural network techniques are implemented.  Here is the code with comments explaining the functionality. </div>
		<pre><code> 
def Train(self):
    global weights0,weights1,layer1_deltaPrev,layer2_deltaPrev
    
    #training iterations are set by the user in the program initialization. 
    for iter in xrange(self.trainingIterations):
   
   	#layer 0 equals the training inputs in NumPy matrix form.  
        layer0 = self.TrainingIn
        
        #neural nets should always use shuffle.  One big bug in this program is that the outputs are in a separate
        #matrix.  This means that randomly shuffling the inputs is going to give them random outputs.  For my training
        #I set shuffle to off.  
        if(self.shuffle):
	    np.random.shuffle(layer0)

	#np.dot is a NumPy multiplication function.  It multiplies layer0(input) by the randomly initialized weights, and is fed through the sigmoid function self.nonlin
        layer1 = self.nonlin(np.dot(layer0,self.weights0))

	#you should always include a bias layer. This means adding a column of 1's to your matrix 
        if(self.biasLayer1TF):
            self.biaslayer1 = np.ones(layer1.shape[0])
            layer1=np.column_stack((layer1,self.biaslayer1))
            
        #dropout should also always be used.  0>dropout<1
        if(self.dropout):
            layer1 *= np.random.binomial([np.ones((layer1.shape[0],layer1.shape[1]))],1-self.dropoutPercent)[0]
        
        #layer2 equals layer1*weights1 fed through the sigmoid function self.nonlin
        #layer2 is what the network is predicting the output will be.  
        layer2 = self.nonlin(np.dot(layer1,self.weights1))
        
        #more on this later
        if(self.miniBtch_StochastGradTrain):
            self.layer1_delta,self.layer2_delta=self.MiniBatchOrStochasticGradientDescent()
            self.layer2_deltaPrev=self.layer2_delta
            self.layer1_deltaPrev=self.layer1_delta
        else:

	    #the error= the distance between the guesses from the actual values.  
            layer2_error = self.TrainingOut - layer2
            
            #take the derivative (slope) of each node in layer2, and multiply it by the error.  The higher the error,
            #the more the slope delta will be.  Learning rate is also applied here, which is how fast you want your network to correct 
            #itself.  Additionally momentum is applied to the end of this, which is added the previous iterations deltas to the current iteration 
            self.layer2_delta = layer2_error*self.nonlin(layer2,deriv=True)*self.learningRate+self.momentum*self.layer2_deltaPrev
            self.layer2_deltaPrev=self.layer2_delta

	    #.T means the transposed matrix in Numpy.  
            layer1_error = self.layer2_delta.dot(self.weights1.T)
            self.layer1_delta = layer1_error * self.nonlin(layer1,deriv=True)*self.learningRate+self.momentum*self.layer1_deltaPrev
            self.layer1_deltaPrev=self.layer1_delta

	#update the weights in each layer with the delta layers
        self.weights1 += layer1.T.dot(self.layer2_delta)
        if(self.biasLayer1TF):
            self.weights0 += layer0.T.dot(self.layer1_delta.T[:-1].T)
        else:
            self.weights0 += layer0.T.dot(self.layer1_delta)
        

        if (iter% 20) == 0:
            self.CalculateValidationError()

            if(self.minimumValidationError<0.01):
                break
		</code></pre>



			

			
			







              <div class="docs-section" style={intro} id="client">
		      <div>I'm going to walkthrough the neural net code above with the following data:</div>
		       <div><img src="mlb pics/input.png" width="20%" height="5%"/></div>
              
		      			<p>Looking at the table we have 4 rows with 3 variables of input data.  The output data is 4 rows with 1 result. 
				So we will have a TrainIn matrix of size 4x3 and a TrainOut matrix of 4x1.    
				The above table looks like the following in a NumPy array:</br>
			TestIn = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ]) </br>
			TestOut = np.array([[0,1,1,0]]).T	</br>
			Note the ".T" in the TestOut variable.  This means transpose the matrix.  It is visualized here as a 1x4 matrix, by transposing it we create a 4X1 matrix, conducive to our input set.</p>
		Since we are working with such a simple dataset I'm going to skip over validation data and test data.  Our test data will simply be our training data.  

<p>Now that we have input data to send to the network, I'm going set the parameters for the network. I do this in main.py.  These parameters get sent to the Neural Network class and initialize the model. </br>
		I'm going to concentrate on the parameters hiddenDim, learningRate, trainingIterations, momentum, and dropout. </br></br>Below is the parameter dictionary I send to the Neural Network class.    </p>
		      <p> </p>
		      
		  	<pre><code>
Run({"hiddenDim":2,"learningRate":.05,"trainingIterations":5000,"momentum":.1,
"dropout":True,"dropoutPercent":0.65,
 "miniBtch_StochastGradTrain":"False","miniBtch_StochastGradTrain_Split":null,"NFoldCrossValidation":False,"N":null#skip these,
 "biasLayer1TF":True,"addInputBias":False,"shuffle":False
})	
			</code></pre>
					      
		       <p>For now I'm going to skip over bias, minibatch training, stochastic gradient descent training, and N-fold cross validation.  I'll talk about these concepts later.  </p>

			
			
			</div>
<h5>Initializing weights and deltas</h5>
<div>In my NeuralNet class __init__ code I initialize the weights based on the hiddenDim variable: 
<pre><code>np.random.seed(1)
self.weights0 = (2*np.random.random((self.TrainingIn.shape[1],self.hiddenDim)) - 1)
self.weights1 = (2*np.random.random((self.hiddenDim+1 if self.biasLayer1TF else self.hiddenDim,1)) - 1)
</code></pre>
	<p>
	np.random.random creates a matrix of random numbers between 0 and 1.  It is best practice to initialize the weights randomly with mean 0, so I multiply these numbers by 2 and subtract 1.  </br>
self.TrainingIn.shape returns the size of the matrix, in our case 4x3.  Therefore self.TrainingIn.shape[1] returns the second number of this value, 3.  </br>
hiddenDim was set to 2 in our initalization parameters above.  So self.weights0 will be a matrix of size 3X2 with random numbers between -1 and 1.  </br>

Since bias is turned off for this example, weights1 will be initialized with a matrix size 2(hiddenDim)x1 with random numbers between -1 and 1.  </br></br>
We also need to intialize "layerx_deltaPrev" matrices for momentum.  </br>np.zeros creates a matrix of 0's.  
<pre><code>self.layer2_deltaPrev=np.zeros(shape=(self.TrainingOut.shape[0],self.weights1.shape[1]))
self.layer1_deltaPrev=np.zeros(shape=(self.TrainingIn.shape[0],self.hiddenDim+1 if self.biasLayer1TF else self.hiddenDim))
</code></pre>

self.layer2_deltaPrev will be a matrix of size 4X1, and self.layer1_deltaPrev will be a matrix of size 4x2. 


	</p>
<h4>Train()</h4>
We're ready to enter the Train() method from above.  We will iterate in the for loop 5000 times to update the weights of the network based on the input data and expected output.</br>
layer0 = self.TrainingIn.  Self explanatory.</br>
Shuffle is rearranging the data into random rows.  </br>
The next line of code is doing a lot: </br>
<pre><code>    
layer1 = self.nonlin(np.dot(layer0,self.weights0)</br>
</code></pre>

np.dot is matrix multiplication.  Multiply the matrix layer0 by self.weights0.  </br>
Remember back to linear algebra, these matrices have to be compatible sizes.  Since layer0 is  size 4x3 and weights0 is size 3x2, we can multiply these together to output a 4x2 matrix.  (For matrix multiplication the middle two numbers have to be the same)</br></br>
The 4x2 layer1 matrix is illustrated in the picture at the top of the page as the circles in the middle depicting the "hidden layer".  The weights are depicted as the lines between the different layers of the  network.  
</br>
Next the resulting 4x2 matrix gets sent through the function nonlin().  Code:

<pre><code>def nonlin(self,x,deriv=False):
	if(deriv==True):
    		return x*(1-x)
	return 1/(1+np.exp(-x))
</code></pre>
This is the sigmoid function.  1/(1+e^(-x)).</br>
Important function.  Google it.   This is our nonlinearity function.  Notice it maps any value to a value between 0 and 1, a probabilistic scale.  Its derivative returns x*(1-x). We will use this when computing the gradient in a few steps. </br> </br>

The next line is indicating whether to add a bias node to the layer.  This is a column of 1's in your matrix.  Bias is good in every neural network, it is used for shifting your activation function to the left or right on the graph.  

<a   href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks">Great link</a></br>

<h5>Dropout</h5>
<pre><code>layer1 *= np.random.binomial([np.ones((layer1.shape[0],layer1.shape[1]))],1-self.dropoutPercent)[0] 
</code></pre>

</br>
The next line of code is similar to the above code, but it's computing the final output layer.  In other words, the model is guessing what the actual output values will be, which in this case is 0, 1, 1, 0.  
<pre><code>layer2 = self.nonlin(np.dot(layer1,self.weights1))
</code></pre>
Layer1 is a 4x2 matrix, and self.weights1 is a 2x1 matrix.  Multiply these together and get a 4x1 matrix.  Notice this is the same size as the output matrix.  Next we will compute the error and deltas and begin the fun stuff, backpropagation.     
<h5>Backpropagation</h5>s
<pre><code>layer2_error = self.TrainingOut - layer2
</code></pre>
The self.TrainingOut matrix is the actual output values for the dataset, 0,1,1,0.  Layer2 is what the neural network guessed.  Subtracting layer2 from TrainingOut will tell the network how far off its guessed values are to the actual values.  

<pre><code>self.layer2_delta = layer2_error*self.nonlin(layer2,deriv=True)*self.learningRate+self.momentum*self.layer2_deltaPrev
</code></pre>
The simple version of the delta, or the error weighted derivative, is layer2_error*self.nonlin(layer2,deriv=True).  You can see that this is taking the slope (derivative) for the weights and multiplying these by the layer2_error.  
</br></br>
self.learningRate is the rate you want your network to update its weights.  It can be between 0 and 1 and the default is 1.  I think of the learning rate as the rate at which which you want your network to abandon its preconceived ideas about the data.  </br>I learned how important learning rate becomes when fine-tuning a pretrained network on new data.  Fine-tuning involves using a pre-existing model such as ResNet or Google's Inception that has weights based on a specific dataset, and training it on your own data.  When you attempt to train your data's new nodes with a high learning rate, the new nodes won't know anything about the data, and will proceed to update all of the pre-trained models learned weights.  This means that all the work the model did to learn its pretrained weights will be wiped out.  
</br> I got slightly fancy and implemented momentum as well.  Momentum is the second part of the code in the line above:</br>
self.momentum*self.layer2_deltaPrev 
</br>The momentum number is also between 0 and 1, and could be thought of as the velocity you want your neural network to learn with the memory of past velocities.  So, if you are going down the gradient, searching for the minimum (as in the picture below), you don't want to rapidly change directions at each step because some directions could lead to local minima.  Momentum smooths out the descent and pushes the network in the direction that most of the weights push it, rather than the current weight.   You do this by adding the weights from the previous iterations delta to the current iteration:</br>

<div><img src="mlb pics/gradientdescent.png" width="60%" height="5%"/></div>

self.layer2_deltaPrev=self.layer2_delta</br>From here it can use the variable layer2_deltaPrev for the next iteration.  </br></br>
Note* Because layer2_error is a 4x1 matrix and layer2 is also a 4x1 matrix, we are taking the Hadamard product, which is multiplying the corresponding cell in each matrix with each other to produce a matrix of the same size.  
The output of layer2_delta will be a 4x1 matrix.  </br></br>
Next we calculate the layer1_error:
<pre><code>layer1_error = self.layer2_delta.dot(self.weights1.T)
</code></pre>
weights1 is a 2x1 matrix.  .T transposes the matrix to get a 1x2 matrix.  Therefore the output size for layer1_error is (4x1)*(1x2)=4x2 matrix.  Which is the size of the weights0 matrix.  </br>
We again do Hadamard, or "elementwise" multiplication to get the layer1_delta.  We do this with the following code: 
<pre><code>self.layer1_delta = layer1_error * self.nonlin(layer1,deriv=True)*self.learningRate+self.momentum*self.layer1_deltaPrev
self.layer1_deltaPrev=self.layer1_delta
</code></pre>
Momentum and learning rate are applied in the same way they were for layer2_delta, and and layer1_deltaPrev is saved off as well.  </br></br>

Now we update the weights of the network, which is what holds the "learned" information about the data:
<pre><code>
self.weights1 += layer1.T.dot(self.layer2_delta)
</code></pre>
layer2_delta=(4x1) shaped matrix.</br>
layer1=(4x2) shaped matrix -> Transposed=(2x4) shaped matrix.  </br>
2x4 matrix * 4x1 matrix=2x1 matrix, or the weights1 shape.  We add these values to the randomized weights so that weights1 can update with numbers that will more accurately guess what the output will be next time it forward propogates.  

self.weights0 += layer0.T.dot(self.layer1_delta)

the differential calculus the network uses to correct itself and
</div>
					      <p>Shuffle:  Randomly rearrange the data rows so that the network is fed new combinations of data on each iteration. </p>






<!-- End Document
  ?????????????????????????????????????????????????? -->
</body>
</html>
